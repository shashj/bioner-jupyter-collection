{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kFIVa292ncS",
        "outputId": "04f86563-4291-46ca-95b9-ae90d01a5279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bioner-jupyter-collection'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 70 (delta 25), reused 47 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (70/70), 38.24 KiB | 12.75 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shashj/bioner-jupyter-collection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/bioner-jupyter-collection/UMLSBert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk0Lh8g23HFC",
        "outputId": "9e1439d7-6f36-47bd-fd48-bbde24b74f0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bioner-jupyter-collection/UMLSBert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Tv5bib3KSy",
        "outputId": "8fbca432-5c67-475e-dd83-f9e471913ec1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bioner-jupyter-collection/UMLSBert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvi0TtR73M5j",
        "outputId": "229e4aa4-8433-4eff-8310-c6de1afa9bd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 27 17:07:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plOzZClG3SHz",
        "outputId": "ed460109-8026-409b-a122-b83d0d108051"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xr57rBv3VIz",
        "outputId": "566c7504-8fd8-411c-94f6-2105dc91e5ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/270.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/270.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j71HJosF3Z3b",
        "outputId": "c04858d5-0cac-4a6c-b72e-a1fc6bdfab27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36dFvaUe3cef",
        "outputId": "d3da9bd4-7f06-465f-8da7-461ebe018efb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m30.7/43.6 kB\u001b[0m \u001b[31m827.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m776.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=f49407424a84a5f8a98ed3c539cd1d1d6572236b3124d6315285b28826019556\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model_train_ncbi.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch--C3mi3gG7",
        "outputId": "3edf06bb-0882-4407-e208-acb864b73952"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-27 17:29:16.591061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-27 17:29:16.591118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-27 17:29:16.592593: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-27 17:29:17.781455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/bioner-jupyter-collection/UMLSBert/model_train_ncbi.py:20: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"seqeval\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "Checking /root/.cache/huggingface/datasets/downloads/19731ebe212fbfd4664ba03f1bd9192242fad9f7d06f0c7c1e4439500c21bb13.63490f48f0656d09ada2c86daed79587694915df82dd311e407b049ad2f525eb.py for additional imports.\n",
            "No config specified, defaulting to the single config: prep_dataset_ncbi/ncbi_disease\n",
            "Loading Dataset Infos from /content/bioner-jupyter-collection/UMLSBert\n",
            "Checking /content/bioner-jupyter-collection/UMLSBert/prep_dataset_ncbi.py for additional imports.\n",
            "Generating dataset prep_dataset_ncbi (/root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0)\n",
            "Downloading and preparing dataset prep_dataset_ncbi/ncbi_disease to /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "https://github.com/spyysalo/ncbi-disease/raw/master/conll/train.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d1cf8cec69c23a6b34c32a4c3cd81c8273880a9e1c3447d47ca90be96dca8d8f.incomplete\n",
            "Downloading data: 1.14MB [00:00, 19.5MB/s]      \n",
            "storing https://github.com/spyysalo/ncbi-disease/raw/master/conll/train.tsv in cache at /root/.cache/huggingface/datasets/downloads/d1cf8cec69c23a6b34c32a4c3cd81c8273880a9e1c3447d47ca90be96dca8d8f\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/d1cf8cec69c23a6b34c32a4c3cd81c8273880a9e1c3447d47ca90be96dca8d8f\n",
            "https://github.com/spyysalo/ncbi-disease/raw/master/conll/devel.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d35a3a5732b07182c92cc3a7f1b3ab156983d83a885a1384fb5094fc8eca5ae7.incomplete\n",
            "Downloading data: 200kB [00:00, 10.5MB/s]        \n",
            "storing https://github.com/spyysalo/ncbi-disease/raw/master/conll/devel.tsv in cache at /root/.cache/huggingface/datasets/downloads/d35a3a5732b07182c92cc3a7f1b3ab156983d83a885a1384fb5094fc8eca5ae7\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/d35a3a5732b07182c92cc3a7f1b3ab156983d83a885a1384fb5094fc8eca5ae7\n",
            "https://github.com/spyysalo/ncbi-disease/raw/master/conll/test.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/39cd2eb58d611982f617aac24752091ba0f3b84d863fd3976c922a7a7b0b26e7.incomplete\n",
            "Downloading data: 206kB [00:00, 10.2MB/s]        \n",
            "storing https://github.com/spyysalo/ncbi-disease/raw/master/conll/test.tsv in cache at /root/.cache/huggingface/datasets/downloads/39cd2eb58d611982f617aac24752091ba0f3b84d863fd3976c922a7a7b0b26e7\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/39cd2eb58d611982f617aac24752091ba0f3b84d863fd3976c922a7a7b0b26e7\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 5366 examples [00:01, 4687.39 examples/s]Done writing 5433 examples in 2355500 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0.incomplete/prep_dataset_ncbi-train-00000-00000-of-NNNNN.arrow.\n",
            "Generating train split: 5433 examples [00:01, 5219.47 examples/s]\n",
            "Renaming 1 shards.\n",
            "Generating validation split\n",
            "Generating validation split: 600 examples [00:00, 3041.78 examples/s]Done writing 924 examples in 413884 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0.incomplete/prep_dataset_ncbi-validation-00000-00000-of-NNNNN.arrow.\n",
            "Generating validation split: 924 examples [00:00, 2999.87 examples/s]\n",
            "Renaming 1 shards.\n",
            "Generating test split\n",
            "Generating test split: 902 examples [00:00, 3593.66 examples/s]Done writing 941 examples in 422826 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0.incomplete/prep_dataset_ncbi-test-00000-00000-of-NNNNN.arrow.\n",
            "Generating test split: 941 examples [00:00, 3369.36 examples/s]\n",
            "Renaming 1 shards.\n",
            "Unable to verify splits sizes.\n",
            "Dataset prep_dataset_ncbi downloaded and prepared to /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0. Subsequent calls will reuse this data.\n",
            "Constructing Dataset for split train, validation, test, from /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0\n",
            "model.safetensors: 100% 438M/438M [00:08<00:00, 53.7MB/s]\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at GanjinZero/UMLSBert_ENG and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map:   0% 0/5433 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/cache-e5ea46138576a6ee.arrow\n",
            "Map: 100% 5433/5433 [00:02<00:00, 2524.26 examples/s]Done writing 5433 examples in 4663066 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/tmprfmkib7n.\n",
            "Finished processing shard number None of 1.\n",
            "Map: 100% 5433/5433 [00:02<00:00, 2570.84 examples/s]\n",
            "Map:   0% 0/924 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/cache-0879390e48d46ad9.arrow\n",
            "Map: 100% 924/924 [00:00<00:00, 2518.70 examples/s]Done writing 924 examples in 813948 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/tmplf2ghcl6.\n",
            "Finished processing shard number None of 1.\n",
            "Map: 100% 924/924 [00:00<00:00, 2482.26 examples/s]\n",
            "Map:   0% 0/941 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/cache-bca6cb97769f3d82.arrow\n",
            "Map: 100% 941/941 [00:00<00:00, 4343.39 examples/s]Done writing 941 examples in 836994 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/tmpfrw1mazk.\n",
            "Finished processing shard number None of 1.\n",
            "Map: 100% 941/941 [00:00<00:00, 4269.20 examples/s]\n",
            "  0% 0/1700 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 20% 339/1700 [00:24<01:34, 14.42it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 6/58 [00:00<00:00, 58.88it/s]\u001b[A\n",
            " 21% 12/58 [00:00<00:00, 56.25it/s]\u001b[A\n",
            " 31% 18/58 [00:00<00:00, 54.74it/s]\u001b[A\n",
            " 41% 24/58 [00:00<00:00, 53.81it/s]\u001b[A\n",
            " 52% 30/58 [00:00<00:00, 54.04it/s]\u001b[A\n",
            " 62% 36/58 [00:00<00:00, 52.18it/s]\u001b[A\n",
            " 72% 42/58 [00:00<00:00, 51.37it/s]\u001b[A\n",
            " 83% 48/58 [00:00<00:00, 48.37it/s]\u001b[A\n",
            " 93% 54/58 [00:01<00:00, 49.45it/s]\u001b[ADone writing 924 examples in 300520 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.04776890203356743, 'eval_precision': 0.8193202146690519, 'eval_recall': 0.8867376573088093, 'eval_f1': 0.8516968851696886, 'eval_accuracy': 0.9857432221875974, 'eval_runtime': 1.7655, 'eval_samples_per_second': 523.366, 'eval_steps_per_second': 32.852, 'epoch': 1.0}\n",
            " 20% 340/1700 [00:26<01:34, 14.42it/s]\n",
            "100% 58/58 [00:01<00:00, 49.45it/s]\u001b[A\n",
            "{'loss': 0.0778, 'learning_rate': 1.4117647058823532e-05, 'epoch': 1.47}\n",
            " 40% 679/1700 [00:58<01:17, 13.21it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 57.17it/s]\u001b[A\n",
            " 22% 13/58 [00:00<00:00, 55.04it/s]\u001b[A\n",
            " 33% 19/58 [00:00<00:00, 54.71it/s]\u001b[A\n",
            " 43% 25/58 [00:00<00:00, 51.75it/s]\u001b[A\n",
            " 53% 31/58 [00:00<00:00, 52.40it/s]\u001b[A\n",
            " 64% 37/58 [00:00<00:00, 50.80it/s]\u001b[A\n",
            " 74% 43/58 [00:00<00:00, 49.36it/s]\u001b[A\n",
            " 83% 48/58 [00:00<00:00, 48.30it/s]\u001b[A\n",
            " 93% 54/58 [00:01<00:00, 49.71it/s]\u001b[ADone writing 924 examples in 299392 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.05923890322446823, 'eval_precision': 0.8371877890841813, 'eval_recall': 0.8760890609874153, 'eval_f1': 0.8561967833491013, 'eval_accuracy': 0.9855095045185416, 'eval_runtime': 1.7438, 'eval_samples_per_second': 529.891, 'eval_steps_per_second': 33.262, 'epoch': 2.0}\n",
            " 40% 680/1700 [01:00<01:17, 13.21it/s]\n",
            "100% 58/58 [00:01<00:00, 49.71it/s]\u001b[A\n",
            "{'loss': 0.0194, 'learning_rate': 8.23529411764706e-06, 'epoch': 2.94}\n",
            " 60% 1019/1700 [01:28<01:04, 10.58it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 69.27it/s]\u001b[A\n",
            " 24% 14/58 [00:00<00:00, 60.23it/s]\u001b[A\n",
            " 36% 21/58 [00:00<00:00, 58.60it/s]\u001b[A\n",
            " 47% 27/58 [00:00<00:00, 55.96it/s]\u001b[A\n",
            " 57% 33/58 [00:00<00:00, 54.41it/s]\u001b[A\n",
            " 67% 39/58 [00:00<00:00, 52.18it/s]\u001b[A\n",
            " 78% 45/58 [00:00<00:00, 50.34it/s]\u001b[A\n",
            " 88% 51/58 [00:00<00:00, 48.79it/s]\u001b[A\n",
            " 98% 57/58 [00:01<00:00, 50.23it/s]\u001b[ADone writing 924 examples in 300240 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.05937149003148079, 'eval_precision': 0.843978102189781, 'eval_recall': 0.8954501452081317, 'eval_f1': 0.868952559887271, 'eval_accuracy': 0.9860937986911811, 'eval_runtime': 1.6949, 'eval_samples_per_second': 545.158, 'eval_steps_per_second': 34.22, 'epoch': 3.0}\n",
            " 60% 1020/1700 [01:30<01:04, 10.58it/s]\n",
            "100% 58/58 [00:01<00:00, 50.23it/s]\u001b[A\n",
            " 80% 1359/1700 [01:53<00:21, 15.52it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 69.38it/s]\u001b[A\n",
            " 24% 14/58 [00:00<00:00, 60.01it/s]\u001b[A\n",
            " 36% 21/58 [00:00<00:00, 57.75it/s]\u001b[A\n",
            " 47% 27/58 [00:00<00:00, 54.70it/s]\u001b[A\n",
            " 57% 33/58 [00:00<00:00, 53.49it/s]\u001b[A\n",
            " 67% 39/58 [00:00<00:00, 51.55it/s]\u001b[A\n",
            " 78% 45/58 [00:00<00:00, 49.93it/s]\u001b[A\n",
            " 88% 51/58 [00:00<00:00, 48.78it/s]\u001b[A\n",
            " 98% 57/58 [00:01<00:00, 50.20it/s]\u001b[ADone writing 924 examples in 299984 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.06654742360115051, 'eval_precision': 0.8570093457943925, 'eval_recall': 0.8877057115198451, 'eval_f1': 0.8720874940561103, 'eval_accuracy': 0.9863275163602369, 'eval_runtime': 1.7034, 'eval_samples_per_second': 542.431, 'eval_steps_per_second': 34.049, 'epoch': 4.0}\n",
            " 80% 1360/1700 [01:55<00:21, 15.52it/s]\n",
            "100% 58/58 [00:01<00:00, 50.20it/s]\u001b[A\n",
            "{'loss': 0.0071, 'learning_rate': 2.3529411764705885e-06, 'epoch': 4.41}\n",
            "100% 1699/1700 [02:24<00:00, 15.20it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 64.24it/s]\u001b[A\n",
            " 24% 14/58 [00:00<00:00, 59.33it/s]\u001b[A\n",
            " 34% 20/58 [00:00<00:00, 56.22it/s]\u001b[A\n",
            " 45% 26/58 [00:00<00:00, 53.87it/s]\u001b[A\n",
            " 55% 32/58 [00:00<00:00, 53.26it/s]\u001b[A\n",
            " 66% 38/58 [00:00<00:00, 51.94it/s]\u001b[A\n",
            " 76% 44/58 [00:00<00:00, 49.86it/s]\u001b[A\n",
            " 86% 50/58 [00:00<00:00, 48.57it/s]\u001b[A\n",
            " 97% 56/58 [00:01<00:00, 49.66it/s]\u001b[ADone writing 924 examples in 299896 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.07328996062278748, 'eval_precision': 0.8603773584905661, 'eval_recall': 0.882865440464666, 'eval_f1': 0.8714763497372193, 'eval_accuracy': 0.9859769398566531, 'eval_runtime': 1.7259, 'eval_samples_per_second': 535.386, 'eval_steps_per_second': 33.606, 'epoch': 5.0}\n",
            "100% 1700/1700 [02:25<00:00, 15.20it/s]\n",
            "100% 58/58 [00:01<00:00, 49.66it/s]\u001b[A\n",
            "{'train_runtime': 145.8539, 'train_samples_per_second': 186.248, 'train_steps_per_second': 11.656, 'train_loss': 0.03114005109843086, 'epoch': 5.0}\n",
            "100% 1700/1700 [02:25<00:00, 11.66it/s]\n",
            " 97% 56/58 [00:01<00:00, 50.03it/s]Done writing 924 examples in 299896 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "100% 58/58 [00:01<00:00, 34.26it/s]\n",
            " 97% 57/59 [00:01<00:00, 49.26it/s]Done writing 941 examples in 316940 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "100% 59/59 [00:01<00:00, 33.87it/s]\n",
            "Done writing 941 examples in 316940 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "{'Disease': {'precision': 0.873481057898499, 'recall': 0.896551724137931, 'f1': 0.8848660391021, 'number': 1363}, 'overall_precision': 0.873481057898499, 'overall_recall': 0.896551724137931, 'overall_f1': 0.8848660391021, 'overall_accuracy': 0.9830241117704499}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V37G5aUY990n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}