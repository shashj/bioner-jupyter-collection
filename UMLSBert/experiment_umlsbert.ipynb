{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Conll data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankjatav/.virtualenvs/bioner/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/shashankjatav/.virtualenvs/bioner/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 1.23M/1.23M [00:03<00:00, 351kB/s]\n",
      "Downloading data: 100%|██████████| 312k/312k [00:00<00:00, 378kB/s]\n",
      "Downloading data: 100%|██████████| 283k/283k [00:00<00:00, 386kB/s]\n",
      "Generating train split: 14041 examples [00:00, 698346.07 examples/s]\n",
      "Generating validation split: 3250 examples [00:00, 584290.10 examples/s]\n",
      "Generating test split: 3453 examples [00:00, 797650.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "conll_data = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_i2b2 = load_dataset(\"text\", data_files=\"../datasets/i2b2/PHI_Processed_data/train.txt\")\n",
    "## clearly not useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the code from https://huggingface.co/datasets/conll2003/blob/main/conll2003.py for prepping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "logger = datasets.logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "_CITATION = \"\"\"\\\n",
    "None right now\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "i2b2 2006\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRAINING_FILE = \"../datasets/i2b2/PHI_Processed_data/train.txt\"\n",
    "_DEV_FILE = \"../datasets/i2b2/PHI_Processed_data/dev.txt\"\n",
    "_TEST_FILE = \"../datasets/i2b2/PHI_Processed_data/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class i2b2deid2006Config(datasets.BuilderConfig):\n",
    "    \"\"\"BuilderConfig for Conll2003\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"BuilderConfig forConll2003.\n",
    "        Args:\n",
    "          **kwargs: keyword arguments forwarded to super.\n",
    "        \"\"\"\n",
    "        super(i2b2deid2006Config, self).__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class i2b2deid2006(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"Conll2003 dataset.\"\"\"\n",
    "\n",
    "    BUILDER_CONFIGS = [\n",
    "        i2b2deid2006Config(name=\"i2b2deid2006\", version=datasets.Version(\"1.0.0\"), description=\"i2b2 deid 2006 dataset\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 *args,\n",
    "                 cache_dir='./',\n",
    "                 train_file=\"train.txt\",\n",
    "                 val_file=\"dev.txt\",\n",
    "                 test_file=\"test.txt\",\n",
    "                 ner_tags=(\"O\",   \"I-PHONE\",   \"I-PATIENT\",   \"I-LOCATION\",   \"I-ID\",   \"I-HOSPITAL\",   \"I-DOCTOR\",   \"I-DATE\",   \"B-PHONE\",   \"B-PATIENT\",   \"B-LOCATION\",   \"B-ID\",   \"B-HOSPITAL\",   \"B-DOCTOR\",   \"B-DATE\", \"B-AGE\"),\n",
    "                 **kwargs):\n",
    "        self._ner_tags = ner_tags\n",
    "        self._train_file = train_file\n",
    "        self._val_file = val_file\n",
    "        self._test_file = test_file\n",
    "        super(i2b2deid2006, self).__init__(*args, cache_dir=cache_dir, **kwargs)\n",
    "\n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"id\": datasets.Value(\"string\"),\n",
    "                    \"tokens\": datasets.Sequence(datasets.Value(\"string\")),\n",
    "                    \"ner_tags\": datasets.Sequence(\n",
    "                        datasets.features.ClassLabel(\n",
    "                            names=sorted(list(self._ner_tags))\n",
    "                        )\n",
    "                    )\n",
    "                }\n",
    "            ),\n",
    "            supervised_keys=None,\n",
    "            homepage=\"\",\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "    \n",
    "    def _split_generators(self):\n",
    "        \"\"\"Returns SplitGenerators.\"\"\"\n",
    "        data_files = {\n",
    "            \"train\": _TRAINING_FILE,\n",
    "            \"dev\": _DEV_FILE,\n",
    "            \"test\": _TEST_FILE,\n",
    "        }\n",
    "\n",
    "        return [\n",
    "            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": data_files[\"train\"]}),\n",
    "            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": data_files[\"dev\"]}),\n",
    "            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": data_files[\"test\"]}),\n",
    "        ]\n",
    "    \n",
    "    def _generate_examples(self, filepath):\n",
    "        logger.info(\"⏳ Generating examples from = %s\", filepath)\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            guid = 0\n",
    "            tokens = []\n",
    "            ner_tags = []\n",
    "            for line in f:\n",
    "                if line == \"\" or line == \"\\n\":\n",
    "                    if tokens:\n",
    "                        yield guid, {\n",
    "                            \"id\": str(guid),\n",
    "                            \"tokens\": tokens,\n",
    "                            \"ner_tags\": ner_tags,\n",
    "                        }\n",
    "                        guid += 1\n",
    "                        tokens = []\n",
    "                        ner_tags = []\n",
    "                else:\n",
    "                    # i2b2 tokens are space separated\n",
    "                    splits = line.split(\" \")\n",
    "                    tokens.append(splits[0])\n",
    "                    ner_tags.append(splits[1].rstrip())\n",
    "            # last example\n",
    "            if tokens:\n",
    "                yield guid, {\n",
    "                    \"id\": str(guid),\n",
    "                    \"tokens\": tokens,\n",
    "                    \"ner_tags\": ner_tags,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
