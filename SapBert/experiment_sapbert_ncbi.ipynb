{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3pB-qEwYMTV",
        "outputId": "2a9c9042-c6da-4c71-c80f-ad1d0bd0ba4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bioner-jupyter-collection'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 164 (delta 78), reused 102 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (164/164), 2.18 MiB | 13.13 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shashj/bioner-jupyter-collection.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/bioner-jupyter-collection/SapBert/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIz5OjruYtNn",
        "outputId": "9ab62f9d-efcd-497b-f522-a2e89cee1446"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bioner-jupyter-collection/SapBert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzT00TPZYxJo",
        "outputId": "96f7d3f6-c559-45a4-c477-8f29012b60f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bioner-jupyter-collection/SapBert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsM5IdMVYzrn",
        "outputId": "3835c397-e07a-4914-a2d7-c5132f94daf4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 24 17:15:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0              26W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install accelerate datasets seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EijtPRV4Y3J9",
        "outputId": "39789428-b34b-42ff-b5ce-4db24b91247b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=e26fa89d5ae34d22b81caa1d1ef599b54ea0194f096d19d0e50ff6dea24e18dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, seqeval, nvidia-cusolver-cu12, datasets, accelerate\n",
            "Successfully installed accelerate-0.28.0 datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 seqeval-1.2.2 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model_train_ncbi.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1TWOm8yZBkc",
        "outputId": "4ca52262-ad5a-4b5a-9879-1dd9a38c9a36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-24 17:18:34.417273: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-24 17:18:34.417383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-24 17:18:34.445790: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-24 17:18:35.682328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/bioner-jupyter-collection/SapBert/model_train_ncbi.py:20: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"seqeval\")\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/1aec4121eb4fe9daae4cf554d4eba49a1785b14600a9a0d7b23a8461c2e5448f.fef66c5ae9c8e5ceca3a430d2ed4bd43a652202ed179be70ab6d1eb4e8460770.py.incomplete\n",
            "Downloading builder script: 6.33kB [00:00, 11.2MB/s]       \n",
            "storing https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py in cache at /root/.cache/huggingface/datasets/downloads/1aec4121eb4fe9daae4cf554d4eba49a1785b14600a9a0d7b23a8461c2e5448f.fef66c5ae9c8e5ceca3a430d2ed4bd43a652202ed179be70ab6d1eb4e8460770.py\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/1aec4121eb4fe9daae4cf554d4eba49a1785b14600a9a0d7b23a8461c2e5448f.fef66c5ae9c8e5ceca3a430d2ed4bd43a652202ed179be70ab6d1eb4e8460770.py\n",
            "Checking /root/.cache/huggingface/datasets/downloads/1aec4121eb4fe9daae4cf554d4eba49a1785b14600a9a0d7b23a8461c2e5448f.fef66c5ae9c8e5ceca3a430d2ed4bd43a652202ed179be70ab6d1eb4e8460770.py for additional imports.\n",
            "Created importable dataset file at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/9642e8a602ba52bd4d8baee1d13b2deb8247d3719041cf02b40bf8367a05aef5/seqeval.py\n",
            "No config specified, defaulting to the single config: prep_dataset_ncbi/ncbi_disease\n",
            "Loading Dataset Infos from /content/bioner-jupyter-collection/SapBert\n",
            "Checking /content/bioner-jupyter-collection/SapBert/prep_dataset_ncbi.py for additional imports.\n",
            "Generating dataset prep_dataset_ncbi (/root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0)\n",
            "Downloading and preparing dataset prep_dataset_ncbi/ncbi_disease to /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "https://github.com/spyysalo/ncbi-disease/raw/master/conll/train.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d1cf8cec69c23a6b34c32a4c3cd81c8273880a9e1c3447d47ca90be96dca8d8f.incomplete\n",
            "Downloading data: 1.14MB [00:00, 20.4MB/s]      \n",
            "storing https://github.com/spyysalo/ncbi-disease/raw/master/conll/train.tsv in cache at /root/.cache/huggingface/datasets/downloads/d1cf8cec69c23a6b34c32a4c3cd81c8273880a9e1c3447d47ca90be96dca8d8f\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/d1cf8cec69c23a6b34c32a4c3cd81c8273880a9e1c3447d47ca90be96dca8d8f\n",
            "https://github.com/spyysalo/ncbi-disease/raw/master/conll/devel.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/d35a3a5732b07182c92cc3a7f1b3ab156983d83a885a1384fb5094fc8eca5ae7.incomplete\n",
            "Downloading data: 200kB [00:00, 9.28MB/s]        \n",
            "storing https://github.com/spyysalo/ncbi-disease/raw/master/conll/devel.tsv in cache at /root/.cache/huggingface/datasets/downloads/d35a3a5732b07182c92cc3a7f1b3ab156983d83a885a1384fb5094fc8eca5ae7\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/d35a3a5732b07182c92cc3a7f1b3ab156983d83a885a1384fb5094fc8eca5ae7\n",
            "https://github.com/spyysalo/ncbi-disease/raw/master/conll/test.tsv not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/39cd2eb58d611982f617aac24752091ba0f3b84d863fd3976c922a7a7b0b26e7.incomplete\n",
            "Downloading data: 206kB [00:00, 10.5MB/s]        \n",
            "storing https://github.com/spyysalo/ncbi-disease/raw/master/conll/test.tsv in cache at /root/.cache/huggingface/datasets/downloads/39cd2eb58d611982f617aac24752091ba0f3b84d863fd3976c922a7a7b0b26e7\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/39cd2eb58d611982f617aac24752091ba0f3b84d863fd3976c922a7a7b0b26e7\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "Generating train split: 4891 examples [00:00, 6180.15 examples/s]Done writing 5433 examples in 2355500 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0.incomplete/prep_dataset_ncbi-train-00000-00000-of-NNNNN.arrow.\n",
            "Generating train split: 5433 examples [00:00, 6004.02 examples/s]\n",
            "Renaming 1 shards.\n",
            "Generating validation split\n",
            "Generating validation split: 626 examples [00:00, 6234.72 examples/s]Done writing 924 examples in 413884 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0.incomplete/prep_dataset_ncbi-validation-00000-00000-of-NNNNN.arrow.\n",
            "Generating validation split: 924 examples [00:00, 5963.19 examples/s]\n",
            "Renaming 1 shards.\n",
            "Generating test split\n",
            "Generating test split: 667 examples [00:00, 6647.06 examples/s]Done writing 941 examples in 422826 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0.incomplete/prep_dataset_ncbi-test-00000-00000-of-NNNNN.arrow.\n",
            "Generating test split: 941 examples [00:00, 6135.04 examples/s]\n",
            "Renaming 1 shards.\n",
            "Unable to verify splits sizes.\n",
            "Dataset prep_dataset_ncbi downloaded and prepared to /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0. Subsequent calls will reuse this data.\n",
            "Constructing Dataset for split train, validation, test, from /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0\n",
            "tokenizer_config.json: 100% 198/198 [00:00<00:00, 922kB/s]\n",
            "config.json: 100% 462/462 [00:00<00:00, 2.07MB/s]\n",
            "vocab.txt: 100% 226k/226k [00:00<00:00, 6.72MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 593kB/s]\n",
            "model.safetensors: 100% 438M/438M [00:02<00:00, 180MB/s]\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cambridgeltl/SapBERT-from-PubMedBERT-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Map:   0% 0/5433 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/cache-f8b15ad0f4f46974.arrow\n",
            "Map: 100% 5433/5433 [00:01<00:00, 3151.58 examples/s]Done writing 5433 examples in 4665768 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/tmplak9lh8z.\n",
            "Finished processing shard number None of 1.\n",
            "Map: 100% 5433/5433 [00:01<00:00, 3481.94 examples/s]\n",
            "Map:   0% 0/924 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/cache-0dd3a69ca092e1dc.arrow\n",
            "Map: 100% 924/924 [00:00<00:00, 4204.49 examples/s]Done writing 924 examples in 813472 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/tmphwpxt8cx.\n",
            "Finished processing shard number None of 1.\n",
            "Map: 100% 924/924 [00:00<00:00, 4127.46 examples/s]\n",
            "Map:   0% 0/941 [00:00<?, ? examples/s]Set __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/cache-8de50318ed5e75c4.arrow\n",
            "Map: 100% 941/941 [00:00<00:00, 4342.75 examples/s]Done writing 941 examples in 836070 bytes /root/.cache/huggingface/datasets/prep_dataset_ncbi/ncbi_disease/1.0.0/tmp_2x0gris.\n",
            "Finished processing shard number None of 1.\n",
            "Map: 100% 941/941 [00:00<00:00, 4249.69 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            " 20% 339/1700 [00:25<01:49, 12.45it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 6/58 [00:00<00:01, 51.31it/s]\u001b[A\n",
            " 21% 12/58 [00:00<00:00, 52.87it/s]\u001b[A\n",
            " 31% 18/58 [00:00<00:00, 49.69it/s]\u001b[A\n",
            " 41% 24/58 [00:00<00:00, 50.42it/s]\u001b[A\n",
            " 52% 30/58 [00:00<00:00, 51.05it/s]\u001b[A\n",
            " 62% 36/58 [00:00<00:00, 49.28it/s]\u001b[A\n",
            " 72% 42/58 [00:00<00:00, 50.02it/s]\u001b[A\n",
            " 83% 48/58 [00:00<00:00, 46.39it/s]\u001b[A\n",
            " 93% 54/58 [00:01<00:00, 47.83it/s]\u001b[ADone writing 924 examples in 300172 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.06078370660543442, 'eval_precision': 0.8100664767331434, 'eval_recall': 0.8116079923882017, 'eval_f1': 0.8108365019011408, 'eval_accuracy': 0.9817848506123723, 'eval_runtime': 1.8266, 'eval_samples_per_second': 505.868, 'eval_steps_per_second': 31.754, 'epoch': 1.0}\n",
            " 20% 340/1700 [00:27<01:49, 12.45it/s]\n",
            "100% 58/58 [00:01<00:00, 47.83it/s]\u001b[A\n",
            "{'loss': 0.1025, 'grad_norm': 2.170022487640381, 'learning_rate': 1.4117647058823532e-05, 'epoch': 1.47}\n",
            " 40% 679/1700 [01:05<01:12, 14.08it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 65.99it/s]\u001b[A\n",
            " 24% 14/58 [00:00<00:00, 56.50it/s]\u001b[A\n",
            " 34% 20/58 [00:00<00:00, 56.09it/s]\u001b[A\n",
            " 45% 26/58 [00:00<00:00, 53.40it/s]\u001b[A\n",
            " 55% 32/58 [00:00<00:00, 52.70it/s]\u001b[A\n",
            " 66% 38/58 [00:00<00:00, 50.43it/s]\u001b[A\n",
            " 76% 44/58 [00:00<00:00, 49.00it/s]\u001b[A\n",
            " 84% 49/58 [00:00<00:00, 46.86it/s]\u001b[A\n",
            " 95% 55/58 [00:01<00:00, 48.13it/s]\u001b[ADone writing 924 examples in 300348 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.05088651925325394, 'eval_precision': 0.8345521023765996, 'eval_recall': 0.8686964795432921, 'eval_f1': 0.8512820512820513, 'eval_accuracy': 0.9854122786488806, 'eval_runtime': 1.7216, 'eval_samples_per_second': 536.707, 'eval_steps_per_second': 33.689, 'epoch': 2.0}\n",
            " 40% 680/1700 [01:07<01:12, 14.08it/s]\n",
            "100% 58/58 [00:01<00:00, 48.13it/s]\u001b[A\n",
            "{'loss': 0.0248, 'grad_norm': 0.1391592025756836, 'learning_rate': 8.23529411764706e-06, 'epoch': 2.94}\n",
            " 60% 1019/1700 [01:43<01:38,  6.90it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 65.89it/s]\u001b[A\n",
            " 24% 14/58 [00:00<00:00, 56.41it/s]\u001b[A\n",
            " 34% 20/58 [00:00<00:00, 55.05it/s]\u001b[A\n",
            " 45% 26/58 [00:00<00:00, 53.20it/s]\u001b[A\n",
            " 55% 32/58 [00:00<00:00, 52.68it/s]\u001b[A\n",
            " 66% 38/58 [00:00<00:00, 51.03it/s]\u001b[A\n",
            " 76% 44/58 [00:00<00:00, 49.11it/s]\u001b[A\n",
            " 84% 49/58 [00:00<00:00, 46.96it/s]\u001b[A\n",
            " 95% 55/58 [00:01<00:00, 48.28it/s]\u001b[ADone writing 924 examples in 300628 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.05645960196852684, 'eval_precision': 0.8411819021237303, 'eval_recall': 0.8667935299714558, 'eval_f1': 0.8537956888472353, 'eval_accuracy': 0.9848662142132771, 'eval_runtime': 1.7123, 'eval_samples_per_second': 539.623, 'eval_steps_per_second': 33.872, 'epoch': 3.0}\n",
            " 60% 1020/1700 [01:44<01:38,  6.90it/s]\n",
            "100% 58/58 [00:01<00:00, 48.28it/s]\u001b[A\n",
            " 80% 1359/1700 [02:09<00:25, 13.57it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 66.49it/s]\u001b[A\n",
            " 24% 14/58 [00:00<00:00, 58.08it/s]\u001b[A\n",
            " 34% 20/58 [00:00<00:00, 56.92it/s]\u001b[A\n",
            " 45% 26/58 [00:00<00:00, 53.55it/s]\u001b[A\n",
            " 55% 32/58 [00:00<00:00, 52.70it/s]\u001b[A\n",
            " 66% 38/58 [00:00<00:00, 49.68it/s]\u001b[A\n",
            " 76% 44/58 [00:00<00:00, 48.08it/s]\u001b[A\n",
            " 84% 49/58 [00:00<00:00, 46.83it/s]\u001b[A\n",
            " 95% 55/58 [00:01<00:00, 48.80it/s]\u001b[ADone writing 924 examples in 300668 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.06174052879214287, 'eval_precision': 0.8564771668219944, 'eval_recall': 0.8744053282588011, 'eval_f1': 0.8653483992467043, 'eval_accuracy': 0.9854512832514236, 'eval_runtime': 1.7324, 'eval_samples_per_second': 533.375, 'eval_steps_per_second': 33.48, 'epoch': 4.0}\n",
            " 80% 1360/1700 [02:11<00:25, 13.57it/s]\n",
            "100% 58/58 [00:01<00:00, 48.80it/s]\u001b[A\n",
            "{'loss': 0.0113, 'grad_norm': 0.3676132261753082, 'learning_rate': 2.3529411764705885e-06, 'epoch': 4.41}\n",
            "100% 1699/1700 [02:45<00:00, 14.76it/s]\n",
            "  0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            " 12% 7/58 [00:00<00:00, 67.20it/s]\u001b[A\n",
            " 24% 14/58 [00:00<00:00, 57.87it/s]\u001b[A\n",
            " 34% 20/58 [00:00<00:00, 55.71it/s]\u001b[A\n",
            " 45% 26/58 [00:00<00:00, 52.86it/s]\u001b[A\n",
            " 55% 32/58 [00:00<00:00, 51.91it/s]\u001b[A\n",
            " 66% 38/58 [00:00<00:00, 50.92it/s]\u001b[A\n",
            " 76% 44/58 [00:00<00:00, 49.43it/s]\u001b[A\n",
            " 84% 49/58 [00:00<00:00, 47.41it/s]\u001b[A\n",
            " 95% 55/58 [00:01<00:00, 48.85it/s]\u001b[ADone writing 924 examples in 300772 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.06684132665395737, 'eval_precision': 0.8474264705882353, 'eval_recall': 0.8772597526165556, 'eval_f1': 0.8620850864890136, 'eval_accuracy': 0.985217255636165, 'eval_runtime': 1.7533, 'eval_samples_per_second': 527.018, 'eval_steps_per_second': 33.081, 'epoch': 5.0}\n",
            "100% 1700/1700 [02:47<00:00, 14.76it/s]\n",
            "100% 58/58 [00:01<00:00, 48.85it/s]\u001b[A\n",
            "{'train_runtime': 167.2069, 'train_samples_per_second': 162.463, 'train_steps_per_second': 10.167, 'train_loss': 0.04153803636046017, 'epoch': 5.0}\n",
            "100% 1700/1700 [02:47<00:00, 10.17it/s]\n",
            " 98% 57/58 [00:01<00:00, 41.95it/s]Done writing 924 examples in 300772 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "100% 58/58 [00:02<00:00, 25.56it/s]\n",
            " 98% 58/59 [00:01<00:00, 48.32it/s]Done writing 941 examples in 318000 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "100% 59/59 [00:01<00:00, 33.16it/s]\n",
            "Done writing 941 examples in 318000 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "{'Disease': {'precision': 0.8697158697158697, 'recall': 0.9067919075144508, 'f1': 0.887866996816413, 'number': 1384}, 'overall_precision': 0.8697158697158697, 'overall_recall': 0.9067919075144508, 'overall_f1': 0.887866996816413, 'overall_accuracy': 0.9833207831325301}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYyErEM-ZMMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}